<!--- SPDX-License-Identifier: Apache-2.0 -->

# Using Python interfaces

Onnx-mlir has runtime utilities to compile and run ONNX models in Python.
These utilities are implemented by the `OnnxMlirCompiler` compiler interface
(include/OnnxMlirCompiler.h) and the `ExecutionSession` class
(src/Runtime/ExecutionSession.hpp).
Both utilities have an associated Python binding generated by

## Configuring the Python interfaces

Using pybind, a C/C++ binary can be directly imported by the Python interpreter.
For onnx-mlir, there are two such libraries, one to compile onnx-mlir models
and one to run the models.

1. The library to compile onnx-mlir models is generated
by `PyOnnxMlirCompiler` (src/Compiler/PyOnnxMlirCompiler.hpp) and build as a shared
library to `build/Debug/lib/PyOnnxMlirCompiler.cpython-<target>.so`.
2. The library to run onnx-mlir models is generated
by by `PyExecutionSession` (src/Runtime/PyExecutionSession.hpp) and built
as a shared library to `build/Debug/lib/PyRuntime.cpython-<target>.so`.

The module can be imported normally by the Python interpreter as long as it is in your
PYTHONPATH. Another alternative is to create a symbolic link to it in your working directory.

```shell
cd <working directory>
ln -s <path to PyRuntime>
ln -s <path to PyOnnxMlirCompiler>
python3
```

# Python interface to run models: PyRuntime

## Running the PyRuntime interface

An ONNX model is a computation graph and it is often the case that the graph
has a single entry point to trigger the computation. Below is an example of doing
inference for a model that has a single entry point.

```python
import numpy as np
from PyRuntime import ExecutionSession

model = 'model.so' # LeNet from ONNX Zoo compiled with onnx-mlir

# Create a session for this model.
session = ExecutionSession(shared_lib_path=model)
# Input and output signatures of the default entry point.
print("input signature in json", session.input_signature())
print("output signature in json",session.output_signature())
# Do inference using the default entry point.
a = np.full((1, 1, 28, 28), 1, np.dtype(np.float32))
outputs = session.run(input=[a])

for output in outputs:
    print(output.shape)
```

In case a computation graph has multiple entry points, users have to set a specific
entry point to do inference. Below is an example of doing inference with multiple
entry points.
```python
import numpy as np
from PyRuntime import ExecutionSession

model = 'multi-entry-points-model.so'

# Create a session for this model.
session = ExecutionSession(shared_lib_path=model, use_default_entry_point=False) # False to manually set an entry point.

# Query entry points in the model.
entry_points = session.entry_points()

for entry_point in entry_points:
  # Set the entry point to do inference.
  session.set_entry_point(name=entry_point)
  # Input and output signatures of the current entry point.
  print("input signature in json", session.input_signature())
  print("output signature in json",session.output_signature())
  # Do inference using the current entry point.
  a = np.arange(10).astype('float32')
  b = np.arange(10).astype('float32')
  outputs = session.run(input=[a, b])
  for output in outputs:
    print(output.shape)
```

## PyRuntime model API
The complete interface to ExecutionSession can be seen in the sources mentioned previously.
However, using the constructor and run method is enough to perform inferences.

```python
def __init__(self, shared_lib_path: str, use_default_entry_point: bool):
    """
    Args:
        shared_lib_path: relative or absolute path to your .so model.
        use_default_entry_point: use the default entry point that is `run_main_graph` or not. Set to True by default.
    """

def run(self, input: List[ndarray]) -> List[ndarray]:
    """
    Args:
        input: A list of NumPy arrays, the inputs of your model.

    Returns:
        A list of NumPy arrays, the outputs of your model.
    """

def input_signature(self) -> str:
    """
    Returns:
        A string containing a JSON representation of the model's input signature.
    """

def output_signature(self) -> str:
    """
    Returns:
        A string containing a JSON representation of the model's output signature.
    """

def entry_points(self) -> List[str]:
    """
    Returns:
        A list of entry point names.
    """

def set_entry_point(self, name: str):
    """
    Args:
        name: an entry point name.
    """
```

# Python interface to compile models: PyOnnxMlirCompiler

## Running the PyOnnxMlirCompiler interface

An ONNX model can be compiled directly using the `onnx-mlir -O3 --EmitLib` command.
The resulting library can then be executed using Python as shown in the previous
sections. At times, it might be convenient to also compile a model directly in Python.
This section explores the Python methods to do so.

```python
from PyOnnxMlirCompiler import OnnxMlirCompiler, OnnxMlirTarget, OnnxMlirOption
# Load onnx model and create Onnx Mlir Compiler object.
# Compiler needs to know where to find its runtime. Set ONNX_MLIR_RUNTIME_DIR
# to proper path, e.g. export ONNX_MLIR_RUNTIME_DIR=../../build/Debug/lib.
file = './mnist.onnx'
compiler = OnnxMlirCompiler(file)
# Set optimization level to -O3 and enable verbose mode.
compiler.set_option(OnnxMlirOption.opt_level, "3")
compiler.set_option(OnnxMlirOption.verbose, "")
print("Compile", file, "with opt level", compiler.get_option(OnnxMlirOption.opt_level), "for arch", compiler.get_option(OnnxMlirOption.target_arch), "and CPU", compiler.get_option(OnnxMlirOption.target_cpu))
# Generate the library file. Success when rc == 0.
rc = compiler.compile('./mnist-O3', OnnxMlirTarget.emit_lib)
model = compiler.get_output_file_name()
if rc:
    print("Failed to compile with error code", rc)
    exit(1)
print("Compiled onnx file", file, "to", model, "with rc", rc)
```

The `PyOnnxMlirCompiler` module exports the `OnnxMlirCompiler` class to drive the
compilation of a ONNX model into an executable model.
Typically, a compiler object is created for a given model by giving it the file name of the ONNX model.
Then, compiler options can be set/reset to steer the compilation to generate the desired executable.
Finally, the compilation itself is performed by calling the `compile()` command where the user passes the output file name (without extension) as well as the desired compilation target.
Typical targets are `emit_lib` or `emit_jni`.

The modules also export two enums that defines which compiler options can be set (`OnnxMlirOption`) and what is the target output of the compilation (`OnnxMlirTarget`).

The `compile` commands returns a return code reflecting the status of the compilation.
A zero value indicates success, and nonzero values reflect the error code.
Because different Operating Systems may have different suffixes for libraries,
the output file name can be retrieved using the `get_output_file_name()` method.

## PyOnnxMlirCompiler model API

The complete interface to OnnxMlirCompiler can be seen in the sources mentioned previously.
However, using the constructor and the methods below are enough to compile models.

```python
def __init__(self, file_name: str):
    """
    Constructor for an ONNX model contained in a file.
    Args:
        file_name: relative or absolute path to your ONNX model.
    """
def __init__(self, input_buffer: void *, buffer_size: int):
    """
    Constructor for an ONNX model contained in an input buffer.
    Args:
        input_buffer: buffer containing the protobuf representation of the model.
        buffer_size: byte size of the input buffer.
    """
def set_option_from_env(self, env_var_ame: str):
    """
    Method to provide the compiler its options via an environment variable.
    Args:
        env_var_ame: name of the environment variable containing the onnx-mlir options.
        All options listed by `onnx-mlir --help` are supported.
    Returns:
        Zero in case of success, error code in case of failure.
    """
def set_option(self, kind: OnnxMlirOption, value: str):
    """
    Method to provide one compiler option.
    Args:
        kind: name of the option being set. Typical values are
        OnnxMlirOption.opt_level (e.g. "0".."5"), target_triple, target_arch,
        target_cpu, target_accel, or verbose.
        value: value of the option being set.
    Returns:
        Zero in case of success, error code in case of failure.
    """
def clear_option(self, kind: OnnxMlirOption):
    """
    Method to clear one compiler option.
    Args:
        kind: name of the option being cleared.
    """
def get_option(self, kind: OnnxMlirOption):
    """
    Method to get one compiler option.
    Args:
        kind: name of the option being retrieved.
    Returns:
        String corresponding to the compiler option's current value.
    """
def compile(self, output_base_name: str, target: OnnxMlirTarget):
    """
    Method to compile a model.
    Args:
        output_base_name: base name (relative or absolute, without suffix)
        where the compiled model should be written into.
        target: target for the compiler's output. Typical values are
        OnnxMlirTarget.emit_lib or emit_jni.
    Returns:
        Zero on success, error code on failure.
    """
def get_output_file_name(self):
    """
    Method to provide the full (absolute or relative) output file name, including
    its suffix.
    Returns:
        String containing the fle name after successful compilation; empty string on failure.
    """
def get_error_message(self):
    """
    Method to provide the compilation error message.
    Returns:
        String containing the error message; empty string on success.
    """
```

This interfaces uses two Python enums to describe the compiler options and targets. The compiler options are `OnnxMlirOption.target_triple`, `target_arch`, `target_cpu`, `target_accel`, `opt_level`, `opt_flag`, `llc_flag`, `llvm_flag`, and `verbose`.
* Target options uses defined LLVM strings to indicate the compilation target.
* The `opt_level` indicates the optimization level, typically indicated as `-O0` to `-O5`. Values passed in the `set_option` should simply be the numeral value, i.e. `0` to `5`.
* Flags options enable the user to pass specific option strings to the optimizer (`opt`) and the llvm `llc` compiler.
* The verbose option enable a verbose mode of the compiler. The `set_option` simply set this boolean flag on, regardless of the passed string value.

The target compiler options are `OnnxMlirTarget.emit_onnx_basic`, `emit_onnxir`, `emit_mlir`, `emit_llvmir`, `emit_obj`, `emit_lib`, and `emit_jni`. Their meaning is
defined by executing `onnx-mlir --help`. Notable values are:
*  `emit_onnxir` to list the onnx operations in the MLIR textual representation,
*  `emit_lib` to generate an executable library of the model, and
*  `emit_jni` to generate a Java jar file of the model.